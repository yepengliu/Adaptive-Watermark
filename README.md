# Adaptive Text Watermark For Large Language Models

Official implementation of the watermarking algorithms presented in the paper:

["Adaptive Text Watermark For Large Language Models"](https://arxiv.org/abs/2401.13927) by Yepeng Liu and Yuheng Bu.

## Introduction
The advancement of Large Language Models (LLMs) has led to increasing concerns about the misuse of AI-generated text, and watermarking for LLM-generated text has emerged as a potential solution. However, it is challenging to generate high-quality watermarked text while maintaining robustness, strong security, and the ability to detect watermarks without prior knowledge of the prompt or model. This paper proposes an adaptive text watermarking strategy to address such a challenge. To improve the text quality and maintain robustness, we adaptively add watermarking to token distributions with high entropy measured by an auxiliary model and keep the low entropy token distributions untouched. For the sake of security and to further minimize the watermark's impact on text quality, instead of using a fixed green/red list generated from a random secret key, which can be vulnerable to decryption and forgery, we adaptively scale up the output logits based on the semantic embedding of previously generated text using a well designed semantic mapping model. Our experiments involving various LLMs demonstrate that our approach achieves comparable robustness performance to existing watermark methods. Additionally, the text generated by our method has perplexity comparable to that of un-watermarked LLMs while maintaining sufficient security.

![overview](https://github.com/yepengliu/adaptive-text-watermark/assets/40141652/82ee9722-6398-405b-b3df-a817c34cf454)


## Installation
```
# download the code
git clone https://github.com/yepengliu/adaptive-text-watermark.git
cd adaptive-text-watermark

# install the required environment 
pip install -r requirements.txt
```

## How to use
### Demo usage: generate a watermarked text given a prompt
```
import torch
from sentence_transformers import SentenceTransformer
from model import TransformModel
from utils import load_model
from watermark import Watermark

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# load watermarking model
watermark_model, watermark_tokenizer = load_model(args.watermark_model)
# load measurement model
measure_model, measure_tokenizer = load_model(args.measure_model)
# load semantic embedding model
embedding_model = SentenceTransformer(args.embedding_model).to(device)
embedding_model.eval()
# load semantic mapping model
transform_model = TransformModel()
transform_model.load_state_dict(torch.load(args.transform_model))
transform_model.to(device)
transform_model.eval()
```
